import os
import h5py
import semver
import numpy as np
from tensorflow import __version__ as tf_ver
from facenet_model import Weights, conv_shape

#check whether using tensorflow 1 or 2 and determine which functions to use 
tf_version = semver.VersionInfo.parse(tf_ver)
if tf_version.major == 1:
    import keras
else:
    import tensorflow.keras as keras


'''
Credit to the CMU OpenFace team for providing open-source pre-trained weights for FaceNet
Weights are derived from training on CASIA-WebFace and FaceScrub datasets (total of 500k images)
'''


def dir_walker(directory, filetype):
    
    '''
    goes through a directory and provides a list of paths to each of its files with a given suffix

    Inputs
    directory -- directory of files of interest
    filetype -- string or tuple of strings of file suffixes to identify within the directory

    Output
    list of paths to every file in the directory with the provided suffix
    '''
    
    #get a list of the filepaths for each .csv file (downloaded from OpenFace) containing the weights for a layer
    filepaths = list()

    # go through every file in the weights directory (passed in)
    for file in os.listdir(directory):
        #decode the filename from the filesystem encoding (returns decoded string) 
        filename = os.fsdecode(file)
        #check if the filetype has the suffix of interest
        if filename.endswith(filetype):
            #append the filepath to the overall directory name/string and add it to the list
            filepath = os.path.join(directory, filename)
            filepaths.append(filepath)
    return filepaths  



def csv_parse(weight_dir, filepaths):
    
    '''
    parses the CSVs of pre-trained weights into a more easily usable hashmap/dictionary

    Inputs
    weight_dir -- directory of the CSV files of pre-trained weights
    filepaths -- list of paths to every individual CSV file (generated by dir_walker function)

    Output
    hashmap/dictionary of all of the pre-trained weights I'll use for my model
    '''

    #initialize collection of weights
    weight_dict = dict()
    paths = dict()

    #process each weight and re-encode them appropriately into the hashmap/dictionary
    for name in Weights:
        if 'bn' in name:
            w = np.genfromtxt(os.path.join(weight_dir, name+'_w.csv'), delimiter=',', dtype=None)
            b = np.genfromtxt(os.path.join(weight_dir, name+'_b.csv'), delimiter=',', dtype=None)
            m = np.genfromtxt(os.path.join(weight_dir, name+'_m.csv'), delimiter=',', dtype=None)
            v = np.genfromtxt(os.path.join(weight_dir, name+'_v.csv'), delimiter=',', dtype=None)
            weight_dict[name] = [w,b,m,v]
        elif 'conv' in name:
            w = np.genfromtxt(os.path.join(weight_dir, name+'_w.csv'), delimiter=',', dtype=None)
            w = np.reshape(w, conv_shape[name])
            w = np.transpose(w, (2,3,1,0))
            b = np.genfromtxt(os.path.join(weight_dir, name+'_b.csv'), delimiter=',', dtype=None)
            weight_dict[name] = [w,b]
        elif 'dense' in name:
            w = np.genfromtxt(os.path.join(weight_dir, name+'_w.csv'), delimiter=',', dtype=None)
            w = np.reshape(w, (128, 736))
            w = np.transpose(w, (1,0))
            b = np.genfromtxt(os.path.join(weight_dir, name+'_b.csv'), delimiter=',', dtype=None) 
            weight_dict[name] = [w,b]
    return weight_dict



def load_weights(weight_dir, filetype = '.csv'):
    
    '''
    looks for every CSV file in the directory of pre-trained weights and compiles all of the wieghts into a hashmap/dictionary

    Inputs
    weight_dir -- directory fo the CSV files of pre-trained weights
    filetype -- file type to look for (defaults/assumes CSV)
  
    Output
    hashmap/dictionary of pre-trained weights
    '''
    
    #get a list of the filepaths for each .csv file (downloaded from OpenFace) containing the weights for a layer
    filepaths = dir_walker(weight_dir, filetype)
    filepaths.sort()

    weight_dict = csv_parse(weight_dir, filepaths)
    return weight_dict


def pretrain_FaceNet(FRmodel, weight_dir, filetype = '.csv'):

    '''
    sets pre-trained weights (downloaded from the internet) to the Face Recognition Model I use

    Inputs
    FRmodel -- the Face Recognition model I use
    weight_dir -- directory fo the CSV files of pre-trained weights
    filetype -- file type to look for (defaults/assumes CSV)

    Output
    same model, but with set (pre-trained) weights
    '''

    #compile the weights (CSVs) into a single dictionary
    weights = Weights
    weights_dict = load_weights(weight_dir, filetype)

    #go through the dictionary and set each weight to the layer with the matching key name
    for name in weights:
        if FRmodel.get_layer(name) != None:
            FRmodel.get_layer(name).set_weights(weights_dict[name])
        elif model.get_layer(name) != None:
            model.get_layer(name).set_weights(weights_dict[name])
